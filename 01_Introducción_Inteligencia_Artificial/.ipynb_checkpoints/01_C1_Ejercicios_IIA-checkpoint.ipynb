{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importar las liberías que voy a usar en la calse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase 1\n",
    "## Ejercicio 1 y 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ejercicio 1\n",
    "\n",
    "a = np.array([[1,2],[3,4]])\n",
    "\n",
    "# calcular norma o = numero de elemoentos diferentes a 0\n",
    "# norma p sumatoria de x1_p\n",
    "# norma infinito maximo valor de la matriz\n",
    "\n",
    "def norm_0(m):\n",
    "    n0 = np.sum(m != 0) \n",
    "    return n0\n",
    "\n",
    "def norm_p(m, p): \n",
    "    xp = np.sum(np.positive(m**p)**(1/p))\n",
    "    return xp\n",
    "\n",
    "def norm_2(m): \n",
    "    xp = np.sum(np.abs(m)**2, axis=1)**(1/2)\n",
    "    return xp\n",
    "\n",
    "def norm_inf(m): \n",
    "    xinf = np.max(m, axis=1)\n",
    "    return xinf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3 4]\n",
      " [5 6 7 8]\n",
      " [9 9 9 9]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 5.47722558, 13.19090596, 18.        ])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(a) # ordena los numeros y arroja los indices correspondientes\n",
    "def ej_2(m):\n",
    "    N_2 = norm_2(m)\n",
    "    N_2_S = np.argsort(N_2)\n",
    "    return m[N_2_S]\n",
    "\n",
    "ej_2(a)\n",
    "\n",
    "b = np.array([[1,2,3,4],[5,6,7,8],[9,9,9,9]])\n",
    "N_2 = norm_2(b)\n",
    "N_2_S = np.argsort(b)\n",
    "\n",
    "\n",
    "\n",
    "print(ej_2(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ejercicio 3\n",
    "\n",
    "class Index(object):\n",
    "    def __init__(self, ids):\n",
    "        print('__init__')\n",
    "        unique_ids = np.unique(ids)\n",
    "        id2idx = np.ones(max(unique_ids) + 1, dtype=np.int64)*-1\n",
    "        id2idx[unique_ids] = np.arange(unique_ids.size)\n",
    "        self.id2idx = id2idx\n",
    "        self.idx2id = unique_ids\n",
    "    \n",
    "    def get_idxs(self, ids):\n",
    "        ids = self.id2idx[ids]\n",
    "        return ids, ids != -1\n",
    "    \n",
    "    def get_ids(self, idxs):\n",
    "        return self.idx2id[idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ejercicio #4: precision, recall y accuracy\n",
    "\"\"\"En clasificación contamos con dos arreglos, la “verdad” y la “predicción”. Cada elemento de los arreglos\n",
    "pueden tomar dos valores, “True” (representado por 1) y “False” (representado por 0). Entonces podemos\n",
    "definir 4 variables:\n",
    "● True Positive (TP): la verdad es 1 y la producción es 1.\n",
    "● True Negative (TN): la verdad es 0 y la predicción es 0.\n",
    "● False Negative (FN): la verdad es 1 y la predicción es 0.\n",
    "● False Positive (FP): la verdad es 0 y la producción es 1.\n",
    "Se definen las siguientes métricas:\n",
    "● Precision = TP / (TP + FP)\n",
    "● Recall = TP / (TP + FN)\n",
    "● Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "Suponer que se tienen 2 arreglos:\n",
    "truth = [1,1,0,1,1,1,0,0,0,1]\n",
    "prediction = [1,1,1,1,0,0,1,1,0,0]\n",
    "Calcular las 3 métricas con Numpy y operaciones vectorizadas.\"\"\"\n",
    "\n",
    "truth = np.array([1,1,0,1,1,1,0,0,0,1])\n",
    "prediction = np.array([1,1,1,1,0,0,1,1,0,0])\n",
    "\n",
    "class Metrics(BaseMetric):\n",
    "    \n",
    "    def __init__(self, truth, prediction):\n",
    "        self.truth = truth\n",
    "        self.prediction = prediction\n",
    "    \n",
    "    def get_precision(self):\n",
    "        values_count = len(truth)\n",
    "        true_positives = np.sum(np.logical_and([self.truth == 1], [self.prediction == 1]))\n",
    "        false_positives = np.sum(np.logical_and([self.truth == 0], [self.prediction == 1]))\n",
    "        true_negatives = np.sum(np.logical_and([self.truth == 0], [self.prediction == 0]))\n",
    "        false_negatives = np.sum(np.logical_and([self.truth == 1], [self.prediction == 0]))\n",
    "        Precision = true_positives / ( true_positives + false_positives)\n",
    "        return Precision\n",
    "    \n",
    "    def get_accuracy(self):\n",
    "        values_count = len(truth)\n",
    "        true_positives = np.sum(np.logical_and([self.truth == 1], [self.prediction == 1]))\n",
    "        false_positives = np.sum(np.logical_and([self.truth == 0], [self.prediction == 1]))\n",
    "        true_negatives = np.sum(np.logical_and([self.truth == 0], [self.prediction == 0]))\n",
    "        false_negatives = np.sum(np.logical_and([self.truth == 1], [self.prediction == 0]))\n",
    "        Accuracy = (true_positives + true_negatives) / values_count\n",
    "        return Accuracy\n",
    "    \n",
    "    def get_recall(self):\n",
    "        values_count = len(truth)\n",
    "        true_positives = np.sum(np.logical_and([self.truth == 1], [self.prediction == 1]))\n",
    "        false_positives = np.sum(np.logical_and([self.truth == 0], [self.prediction == 1]))\n",
    "        true_negatives = np.sum(np.logical_and([self.truth == 0], [self.prediction == 0]))\n",
    "        false_negatives = np.sum(np.logical_and([self.truth == 1], [self.prediction == 0]))\n",
    "        Recall = true_positives / (true_positives + false_negatives)\n",
    "        return Recall\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "### Ejercicio #5: average query precision\n",
    "\n",
    "\"\"\"En information retrieval o search engine, en general contamos con queries “q” y para cada “q” una lista de\n",
    "documentos que son verdaderamente relevantes. Para evaluar search engine, es común utilizar la métrica\n",
    "average query precision. Dado un search engine y una lista de queries “q” para evaluación, podemos obtener\n",
    "los siguientes resultados:\n",
    "q_id = [1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4]\n",
    "predicted_rank = [0, 1, 2, 3, 0, 1, 2, 0, 1, 2, 3, 4, 0, 1, 2, 3]\n",
    "truth_relevance = [T, F, T, F, T, T, T, F, F, F, F, F, T, F, F, T ]\n",
    "precision para q_id 1 = 2 / 4\n",
    "precision para q_id 2 = 3 / 3\n",
    "precision para q_id 3 = 0 / 5\n",
    "precision para q_id 4 = 2 / 4\n",
    "average query precision = ((2/4) + (3/3) + (0/5) + (2/4)) / 4\n",
    "Calcular la métricas con Numpy y operaciones vectorizadas.\"\"\"\n",
    "\n",
    "T = True\n",
    "F = False\n",
    "\n",
    "q_id = np.array([1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4])\n",
    "predicted_rank = np.array([0, 1, 2, 3, 0, 1, 2, 0, 1, 2, 3, 4, 0, 1, 2, 3])\n",
    "truth_relevance = np.array([T, F, T, F, T, T, T, F, F, F, F, F, T, F, F, T ])\n",
    "\n",
    "max_query = max(q_id)\n",
    "\n",
    "# Modo List Comprehension\n",
    "\n",
    "query_truth = [np.sum(truth_relevance[(q_id == i)]) for i in range(1,max_query+1)]\n",
    "query_fetched = [len(truth_relevance[(q_id == i)]) for i in range(1,max_query+1)]\n",
    "query_precision = np.divide(query_truth, query_fetched)\n",
    "avg_q_precision = np.average(query_precision)\n",
    "\n",
    "\n",
    "# Modo Vectorizado\n",
    "\n",
    "# filtrar y contar cantidad de resultados relevantes por query \n",
    "truth_relevance_mask = (truth_relevance == T)\n",
    "filtered_true_query_id = q_id[truth_relevance_mask]\n",
    "filtered_true_relevance_count = np.bincount(filtered_true_query_id)\n",
    "\n",
    "# generar la lista correcta de relevance counts\n",
    "unique_query_ids = np.unique(q_id)\n",
    "non_zero_count_idx = np.where(filtered_true_relevance_count > 0)\n",
    "true_relevance_counts = np.zeros(max(unique_query_ids)+1)\n",
    "true_relevance_counts[non_zero_count_idx] = filtered_true_relevance_count[non_zero_count_idx]\n",
    "\n",
    "# obtener la el conteo sólo para queries existentes\n",
    "true_relevance_counts_by_query = true_relevance_counts[unique_query_ids]\n",
    "\n",
    "#get the count of fetched documents\n",
    "fetched_documents_count = np.bincount(q_id)[unique_query_ids]\n",
    "\n",
    "#precision\n",
    "precision_by_query = true_relevance_counts_by_query / fetched_documents_count\n",
    "\n",
    "print(np.mean(precision_by_query))\n",
    "print(avg_q_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Ejercicio #6: average query precision at K\n",
    "En information retrieval o search engine, en general contamos con queries “q” y para cada “q” una lista de\n",
    "documentos que son verdaderamente relevantes. Para evaluar search engine, es común utilizar la métrica\n",
    "average query precision at K. Dado un search engine y una lista de queries “q” para evaluación, podemos\n",
    "obtener los siguientes resultados:\n",
    "k = 3\n",
    "q_id = [1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4]\n",
    "predicted_rank = [0, 1, 2, 3, 0, 1, 2, 0, 1, 2, 3, 4, 0, 1, 2, 3]\n",
    "truth_relevance = [T, F, T, F, T, T, T, F, F, F, F, F, T, F, F, T ]\n",
    "precision para q_id 1 = 2 / 3\n",
    "precision para q_id 2 = 3 / 3\n",
    "precision para q_id 3 = 0 / 3\n",
    "precision para q_id 4 = 1 / 3\n",
    "average query precision at K = ((2/3) + (3/3) + (0/3) + (1/3)) / 4\n",
    "Calcular la métricas con Numpy y operaciones vectorizadas.\"\"\"\n",
    "T = True\n",
    "F = False\n",
    "\n",
    "q_id = [1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4]\n",
    "predicted_rank = [0, 1, 2, 3, 0, 1, 2, 0, 1, 2, 3, 4, 0, 1, 2, 3]\n",
    "truth_relevance = [T, F, T, F, T, T, T, F, F, F, F, F, T, F, F, T ]\n",
    "q_id = np.array(q_id)\n",
    "predicted_rank = np.array(predicted_rank)\n",
    "truth_relevance = np.array(truth_relevance)\n",
    "k = 3\n",
    "\n",
    "# generate masks for the true vales\n",
    "truth_relevance_mask = truth_relevance == True\n",
    "unique_ids = np.unique(q_id)\n",
    "predicted_rank_K_mask = predicted_rank < k\n",
    "truth_relevance_l_k_mask = np.logical_and(predicted_rank_K_mask, truth_relevance_mask)\n",
    "\n",
    "# Filter relevant queries for bincount\n",
    "\n",
    "relevant_filtered_queries = q_id[truth_relevance_l_k_mask]\n",
    "\n",
    "# Bincunt! count the true values per query\n",
    "relevant_count_query_ALL = np.bincount(relevant_filtered_queries)\n",
    "\n",
    "relevant_count_query = relevant_count_query_ALL[unique_ids]\n",
    "\n",
    "# Calculate query i precision!\n",
    "query_precision = relevant_count_query / k\n",
    "\n",
    "# Calculate avg. query precision\n",
    "\n",
    "avg_query_precision = np.mean(query_precision)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.4\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Ejercicio #7: computar todas las métricas con __call__\n",
    "En problemas de machine learning, es muy común que para cada predicción que obtenemos en nuestro\n",
    "dataset de verificacion y evaluacion, almacenemos en arreglos de numpy array el resultado de dicha\n",
    "predicción, junto con el valor verdadero y parámetros auxiliares (como el ranking de la predicción y el query\n",
    "id).\n",
    "Luego de obtener todas las predicciones, podemos utilizar la información almacenada en los arreglos de\n",
    "numpy, para calcular todas las métricas que queremos medir en nuestro sistema.\n",
    "Una buena práctica para implementar esto en Python, es crear clases que hereden de una clase Metric\n",
    "“base” y que cada métrica implemente el método __call__.\n",
    "Utilizar herencia, operador __call__, kwargs, para escribir un programa que permita calcular todas las\n",
    "métricas de los ejercicios anteriores mediante un for. Cual es la ventaja de resolver el problema utilizando\n",
    "estas herramientas?\"\"\"\n",
    "\n",
    "\n",
    "truth = np.array([1,1,0,1,1,1,0,0,0,1])\n",
    "prediction = np.array([1,1,1,1,0,0,1,1,0,0])\n",
    "\n",
    "class BaseMetric:\n",
    "    def __init__(self, truth, prediction):\n",
    "        self.truth = truth\n",
    "        self.prediction = prediction\n",
    "        \n",
    "        self.true_positives = np.sum(np.logical_and([truth == 1], [prediction == 1]))\n",
    "        self.false_positives = np.sum(np.logical_and([truth == 0], [prediction == 1]))\n",
    "        self.true_negatives = np.sum(np.logical_and([truth == 0], [prediction == 0]))\n",
    "        self.false_negatives = np.sum(np.logical_and([truth == 1], [prediction == 0]))\n",
    "\n",
    "class Precision(BaseMetric):\n",
    "    \n",
    "    def __call__(self):\n",
    "        values_count = len(self.truth)\n",
    "        Precision = self.true_positives / ( self.true_positives + self.false_positives)\n",
    "        return Precision\n",
    "    \n",
    "class Accuracy(BaseMetric):\n",
    "    def __call__(self):\n",
    "        values_count = len(self.truth)\n",
    "        Accuracy = (self.true_positives + self.true_negatives) / values_count\n",
    "        return Accuracy\n",
    "    \n",
    "class Recall(BaseMetric):\n",
    "    \n",
    "    def __call__(self):\n",
    "        values_count = len(self.truth)\n",
    "        Recall = self.true_positives / (self.true_positives + self.false_negatives)\n",
    "        return Recall\n",
    "    \n",
    "\n",
    "metrics = [BaseMetric(truth, prediction), Accuracy(truth, prediction), Recall(truth, prediction)]\n",
    "\n",
    "for metric in metrics:\n",
    "    print(metric())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Ejercicio #8: transformar un dataset a numpy estructurado con singleton\n",
    "Descargar un dataset de ejemplo de la siguiente URL:\n",
    "https://www.kaggle.com/rounakbanik/the-movies-dataset/data?select=ratings.csv\n",
    "Crear la cuent en Kaggle, porque es un lugar de donde potencialmente vamos a descargar muchos recursos.\n",
    "Crear un array estructurado en numpy que represente correctamente la estructura del dataset (“userId”,\n",
    "“movieId”, “rating”, “timestamp”).\n",
    "Crear una clase que permita:\n",
    "OK- Crear la estructura de un structured numpy array para el dataset.\n",
    "OK- Leer el csv, almacenar la información en el array estructurado.\n",
    "OK Guardar el array estructurado en formato .pkl\n",
    "OK Crear una instancia singleton del array estructurado (utilizando __new__ e __init__).\n",
    "● Al crear la instancia, si se encuentra el .pkl cargar desde el pkl. Si el .pkl no está, comenzar por\n",
    "transformar el .csv en .pkl y luego levantar la información.\n",
    "● Encontrar una forma de optimizar la operación usando generators.\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Constants\n",
    "\n",
    "CSV_PATH = \"C:/Users/jota_/00_Especialización_IA/00_Recursos/01_DataSets/ratings-MINI.csv\"\n",
    "PICKLE_PATH = 'C:/Users/jota_/00_Especialización_IA/00_Recursos/03_Pickled/ratings-MINI.pickle'\n",
    "\n",
    "\n",
    "# Create Numpy Structure\n",
    "\n",
    "struct_array = np.array([], \n",
    "                        dtype=[('userId',np.int32), \n",
    "                               ('movieId',np.int32) ,\n",
    "                               ('rating',np.float32),\n",
    "                               ('timestamp', np.float32)])\n",
    "\n",
    "\n",
    "# Crear singleton\n",
    "\n",
    "class structured_array_import(object):\n",
    "    instance = None\n",
    "    \n",
    "    def __new__(cls, struct_array, CSV_PATH, PICKLE_PATH):\n",
    "        if structured_array_import.instance is None:\n",
    "            print('__new__ object created')\n",
    "            structured_array_import.instance = super(structured_array_import, cls).__new__(cls)\n",
    "            return structured_array_import.instance\n",
    "        else: \n",
    "            return structured_array_import.instance\n",
    "                                                                \n",
    "    \n",
    "    def __init__(self, struct_array, CSV_PATH, PICKLE_PATH):\n",
    "        self.struct_array = struct_array\n",
    "        self.CSV_PATH = CSV_PATH\n",
    "        self.PICKLE_PATH = PICKLE_PATH\n",
    "        \n",
    "    \n",
    "    def check_pickle(self):\n",
    "        if os.path.getsize(self.PICKLE_PATH) > 0:\n",
    "            return True\n",
    "        elif os.path.exists(self.PICKLE_PATH):\n",
    "            print('the pickle file exists but is empty')\n",
    "            return False\n",
    "        else:\n",
    "            print('there is no pickle file')\n",
    "            return False\n",
    "        \n",
    "    def load_pickle(self):    \n",
    "        with open(self.PICKLE_PATH, 'rb') as f:\n",
    "                            full_array = pickle.load(f)\n",
    "                            print('full array is', full_array)\n",
    "                            return full_array\n",
    "    \n",
    "    \n",
    "    def load_file(self):\n",
    "        \n",
    "        \"\"\" This function will check if the corresponding pickle file exists. Is if does exist, \n",
    "        it will load the file from PICKLE, if it does not, it will load it throught the csv and \n",
    "        later save it in a pickle. \"\"\"\n",
    "        \n",
    "        \n",
    "        full_array = self.struct_array\n",
    "        # Read CSV\n",
    "        \n",
    "        if self.check_pickle() == False:\n",
    "        \n",
    "            with open(self.CSV_PATH, 'r') as file:\n",
    "                c = 0\n",
    "\n",
    "                # Read CSV\n",
    "\n",
    "                for row in file:\n",
    "                    c += 1\n",
    "                    raw_row = np.array([row.strip('\\n').split(',')])\n",
    "                    row_ful = np.zeros([1],dtype=self.struct_array.dtype)\n",
    "\n",
    "                    # Fill struct array with CSV info.. This will CRUSH!! It does not adapt\n",
    "                    # to new array structures.. :(\n",
    "                    try:\n",
    "                        row_ful[0][0] = raw_row[0][0].astype(np.int32) \n",
    "                        row_ful[0][1] = raw_row[0][1].astype(np.int32)\n",
    "                        row_ful[0][2] = raw_row[0][2].astype(np.float32)\n",
    "                        row_ful[0][3] = raw_row[0][3].astype(np.int32)\n",
    "\n",
    "                        full_array = np.hstack((full_array, row_ful))\n",
    "\n",
    "                    except:    \n",
    "                        print(\"Se omitió la fila {}\".format(c))\n",
    "\n",
    "            with open(PICKLE_PATH, 'wb') as f:\n",
    "                pickle.dump(full_array, f)\n",
    "\n",
    "            return full_array\n",
    "        else:\n",
    "            full_array = self.load_pickle()\n",
    "            return full_array\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 283.090454,
   "position": {
    "height": "40px",
    "left": "1145.36px",
    "right": "20px",
    "top": "9px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
